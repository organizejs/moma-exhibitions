{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup, Comment\n",
    "from itertools import compress\n",
    "from joblib import Parallel, delayed\n",
    "from ethnicolr import census_ln, pred_census_ln, pred_wiki_name\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import util\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoMA exhibits & Artists Exhibited at the MoMA\n",
    "This script is used to build the datset of exhibitions and artists exhibited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There have been 4968 exhibitions archived on the MoMA's website as of 4/7/2018\n",
    "total_exhibitions = 4968\n",
    "\n",
    "# print progress and errors when scaping data\n",
    "print_progress_and_errors = False\n",
    "\n",
    "# scrape a small set of sample data instead of the full data set that would take several hours\n",
    "pull_sample_data = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions for parsing through scraped strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strip_html(text):\n",
    "    return re.sub('<[^<]+?>', '', text).strip()\n",
    "\n",
    "def _strip_non_numbers(text):\n",
    "    return re.sub(\"[^0-9]\", \"\", text).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_parse_exhibit` is used to parse through each exhibit on the MoMA's website. For each exhibit, the function will find key information about the exhibit. This function will also call on the `_parse_exhibit_artists` which will get the list of all artists who participated in the exhibit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_exhibit(exhibit_index, print_progress=True):\n",
    "    \"\"\"\n",
    "    Get the following attributes of the exhibit:\n",
    "    - name of exhibit\n",
    "    - date text of exhibit\n",
    "    - year of exhibit\n",
    "    - which museum (MoMA, MoMA PS1, online...)\n",
    "    - press release text\n",
    "    - list of artists (indexed on artist full name)\n",
    "    \"\"\"\n",
    "\n",
    "    if print_progress == True:\n",
    "        if (exhibit_index % 10) == 0:\n",
    "            print(str(exhibit_index) + ', ', end='')\n",
    "    \n",
    "    try:\n",
    "        page = requests.get(\"https://www.moma.org/calendar/exhibitions/%s\"%(exhibit_index))\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        # get exhibit title\n",
    "        title = _strip_html(str(soup.find('h1', {'class': 'page-header__title'})))\n",
    "        \n",
    "        # get exhibit year and date_full_text\n",
    "        date_full_text = _strip_html(str(soup.find('h2', {'class': 'page-header__subheading--narrow'})))\n",
    "        year = ''\n",
    "        r = re.findall('.*([1-3][0-9]{3})', date_full_text)\n",
    "        if r:\n",
    "            year = str(r.pop())\n",
    "        else:\n",
    "            year = '-1'\n",
    "        \n",
    "        # get which museum (MoMA, PS1, Online...)\n",
    "        museum = _strip_html(str(soup.find('p', {'class': 'calendar-tile__location--title center'})))\n",
    "        \n",
    "        # get press release\n",
    "        press_release_container = soup.find('div', {'class': 'container-uneven--2 body-copy--simple'})\n",
    "        press_release = \" \".join([str(text) for text in press_release_container.find_all('p')])\n",
    "    \n",
    "        # get artists with _parse_exhibit_artists\n",
    "        exhibit_artists_dict = _parse_exhibit_artists(exhibit_index)\n",
    "        artists = \", \".join(list(exhibit_artists_dict['artist_name']))\n",
    "\n",
    "        return title, year, date_full_text, museum, press_release, artists, exhibit_artists_dict\n",
    "    \n",
    "    except Exception:\n",
    "        if print_progress == True:\n",
    "            print(\"Error[%i], \"%exhibit_index, end='')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_parse_exhibit_artists` function will parse through each artist in a particular exhibit, and retreive specific information for each artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_exhibit_artists(exhibit_index):\n",
    "    \"\"\"\n",
    "    Get the follow attributes of the artists in an exhibit:\n",
    "    - name of artist\n",
    "    - associated gender & ethnicity\n",
    "    - nationality\n",
    "    - number of exhibitions\n",
    "    - number of \"works online\"\n",
    "    \"\"\"\n",
    "    page = requests.get(\"https://www.moma.org/artists?exhibition_id=%s\"%(exhibit_index))\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    artist_tiles = soup.find(\"div\", {\"class\": \"tile-container\"})\n",
    "    \n",
    "    try:\n",
    "        artist_names = artist_tiles.findAll(\"div\", {\"class\": \"caption--artist__name center balance-text\"})\n",
    "        artist_nationalities_and_dates = artist_tiles.findAll(\"div\", {\"class\": \"caption--artist__date center balance-text\"})\n",
    "        artist_exhibitions_and_work_online = artist_tiles.findAll(\"div\", {\"class\": \"caption--artist__count center\"})\n",
    "\n",
    "        name_arr, nationality_arr, exhibitions_arr, work_online_arr = ([] for i in range(4))\n",
    "        for i in range(len(artist_names)):\n",
    "            # get name\n",
    "            name = _strip_html(str(artist_names[i]))\n",
    "\n",
    "            # get nationality if available, None otherwise\n",
    "            nationality_raw = _strip_html(str(artist_nationalities_and_dates[i])).split(', ')\n",
    "            nationality_filter = [x.isalnum() for x in nationality_raw]\n",
    "            nationality_list = list(compress(nationality_raw, nationality_filter))\n",
    "            nationality = nationality_list[0] if len(nationality_list) > 0 else \"\"\n",
    "\n",
    "            exhibitions_and_work_online_raw = _strip_html(str(artist_exhibitions_and_work_online[i])).split(', ')\n",
    "\n",
    "            exhibitions_filter = [\"exhibition\" in x for x in exhibitions_and_work_online_raw]\n",
    "            exhibitions_list = list(compress(exhibitions_and_work_online_raw, nationality_filter))\n",
    "            exhibitions = _strip_non_numbers(exhibitions_list[0]) if len(exhibitions_list) > 0 else 0\n",
    "\n",
    "            work_online_filter = [\"online\" in x for x in exhibitions_and_work_online_raw]\n",
    "            work_online_list = list(compress(exhibitions_and_work_online_raw, work_online_filter))\n",
    "            work_online = _strip_non_numbers(work_online_list[0]) if len(work_online_list) > 0 else 0\n",
    "\n",
    "            name_arr.append(name)\n",
    "            nationality_arr.append(nationality)\n",
    "            exhibitions_arr.append(exhibitions)\n",
    "            work_online_arr.append(work_online)\n",
    "\n",
    "        return {\n",
    "            'artist_name': name_arr,\n",
    "            'nationality': nationality_arr,\n",
    "            'exhibitions': exhibitions_arr,\n",
    "            'work_online': work_online_arr\n",
    "        }\n",
    "    \n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will loop through exhibit indexes 1 to `total_exhibitions`, running the `_parse_exhibit` and `_parse_exhibit_artists` for each index. For each index, we will hit the exhibition url and the artist-by-exhibition url.\n",
    "\n",
    "WARNING: This function is extremely costly, taking up to 3-4 hours to execute (on a single thread)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 17.17165780067444 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\"\"\" COMPUTATIONALLY EXPENSIVE \"\"\"\n",
    "# build exhibit dataframe\n",
    "start_time = time.time()\n",
    "\n",
    "# using joblib [not fully tested]\n",
    "# exhibits = Parallel(n_jobs=2)(delayed(_parse_exhibit)(j+1) for j in range(50))\n",
    "\n",
    "if pull_sample_data:\n",
    "    exhibits = [_parse_exhibit(j+1, print_progress_and_errors) for j in range(100, 110)]\n",
    "else:\n",
    "    exhibits = [_parse_exhibit(j+1, print_progress_and_errors) for j in range(total_exhibitions - 1)]\n",
    "\n",
    "exhibits = [exhibit for exhibit in exhibits if exhibit is not None]\n",
    "\n",
    "titles = [exhibit[0] for exhibit in exhibits]\n",
    "years = [exhibit[1] for exhibit in exhibits]\n",
    "date_full_text = [exhibit[2] for exhibit in exhibits]\n",
    "museum = [exhibit[3] for exhibit in exhibits]\n",
    "press_release = [exhibit[4] for exhibit in exhibits]\n",
    "artists = [exhibit[5] for exhibit in exhibits]\n",
    "artist_dict = [exhibit[6] for exhibit in exhibits]\n",
    "\n",
    "exhibition_df = pd.DataFrame(data={\n",
    "    'exhibition_title': pd.Series(titles, dtype=str),\n",
    "    'year': pd.Series(years, dtype=int),\n",
    "    'date_full_text': pd.Series(date_full_text, dtype=str),\n",
    "    'artists': pd.Series(artists, dtype=str),\n",
    "    'museum': pd.Series(museum, dtype=str),\n",
    "    'press_release': pd.Series(press_release, dtype=str)\n",
    "})\n",
    "\n",
    "artist_df = pd.DataFrame()\n",
    "for d in artist_dict:\n",
    "    artist_df = artist_df.append(pd.DataFrame(d))\n",
    "artist_df = artist_df.reset_index().drop(\"index\", axis=1)\n",
    "    \n",
    "print()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up `artist_df` and assign a race using __ethnicolr__. This assigns a race and race-probabiliy for each artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split artist name into first_name and last_name\n",
    "# if last name is not there, use first name as last name\n",
    "artist_df['first_name'] = artist_df['artist_name'].apply(lambda x: x.split(' ', 1)[0])\n",
    "artist_df['last_name'] = artist_df['artist_name'].apply(lambda x: x.split(' ', 1)[1] if len(x.split(' ', 1)) > 1 else x.split(' ', 1)[0])\n",
    "\n",
    "# use pred_wiki_ln\n",
    "artist_df = pred_wiki_name(artist_df, lname_col=\"last_name\", fname_col=\"first_name\")\n",
    "\n",
    "# drop pcts and rename race to ethnicity\n",
    "artist_df = artist_df.drop(list(artist_df.columns)[-13:], axis=1)\n",
    "artist_df.rename(index=str, columns={\"race\": \"ethnicity\"}, inplace=True)\n",
    "\n",
    "def generalize_race(text):\n",
    "    if \"EastAsian\" in text:\n",
    "        return \"asian\"\n",
    "    elif \"Indian\" in text:\n",
    "        return \"indian\"\n",
    "    elif \"African\" in text:\n",
    "        return \"black\"\n",
    "    elif \"Hispanic\" in text:\n",
    "        return \"hispanic\"\n",
    "    else:\n",
    "        return \"white\"\n",
    "    \n",
    "artist_df['race'] = artist_df['ethnicity'].apply(generalize_race)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Genderize.io, run all names through their API to get the gender and gender probability of each artist. When using Genderize.io, we will run small batch sizes of 10 names per call to reduce the number of calls. (10 happens to be the limit for their API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiata/anaconda3/envs/ml/lib/python3.6/site-packages/pandas/core/indexes/api.py:77: RuntimeWarning: '<' not supported between instances of 'str' and 'int', sort order is undefined for incomparable objects\n",
      "  result = result.union(other)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" COMPUTATIONALLY EXPENSIVE ($)\"\"\"\n",
    "# this operation requires using the genderize.io end point - which costs money...\n",
    "def get_genders(names):\n",
    "    url = \"\"\n",
    "    cnt = 0\n",
    "    if not isinstance(names,list):\n",
    "        names = [names,]\n",
    "\n",
    "    for name in names:\n",
    "        # scrub name\n",
    "        if str(name) == None or str(name) == 'nan':\n",
    "            name = \"Unknown\"\n",
    "        else:\n",
    "            name = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", name).strip()\n",
    "            name = re.sub('[^A-Za-z0-9]+', ' ', name).replace(\"&amp;\", \"\")\n",
    "        \n",
    "        if url == \"\":\n",
    "            url = \"name[0]=\" + name\n",
    "        else:\n",
    "            cnt += 1\n",
    "            url = url + \"&name[\" + str(cnt) + \"]=\" + name\n",
    "\n",
    "    key = \"7dc4df2d75de0a9624773093c6717b50\"\n",
    "    req = requests.get(\"https://api.genderize.io/?\" + url + \"&apikey=\"+key)\n",
    "    results = json.loads(req.text)\n",
    "    \n",
    "    retrn = []\n",
    "    for result in results:\n",
    "        if result[\"gender\"] is not None:\n",
    "            retrn.append((result[\"gender\"], result[\"probability\"]))\n",
    "        else:\n",
    "            retrn.append((u'None',u'0.0'))\n",
    "    return retrn    \n",
    "\n",
    "\n",
    "# batch calls to get_genders() into name groupings of len=10 (to minimize times it needs to hit the endpoint)\n",
    "l = len(artist_df)\n",
    "remainder = l % 10\n",
    "artist_genders = []\n",
    "for i in range(1, math.floor(l/10) + 1):\n",
    "    next_10_artists = artist_df.iloc[int(i*10 - 10):int(i*10), 4] # column index 4 is first_name\n",
    "    artist_genders.extend(get_genders(list(next_10_artists)))\n",
    "    if i == math.floor(l/10):\n",
    "        remaining_artist = artist_df.iloc[int(i*10):int(i*10 + remainder + 1), 4] # column index 4 is first_name\n",
    "        artist_genders.extend(get_genders(list(remaining_artist)))\n",
    "        \n",
    "# save genders/gender_prob as separate dataframe\n",
    "gender_df = pd.DataFrame(artist_genders, columns=[\"gender\", \"gender_prob\"])\n",
    "\n",
    "# attach genders dataframe to artist_df\n",
    "artist_df = pd.concat([artist_df, gender_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicates and clean up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_df = artist_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_df = artist_df.drop(['gender_prob', 'first_name', 'last_name'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only looking at American artists, we'll try to get a higher accuracy by looking specifically at US Census data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lastname</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>mix</th>\n",
       "      <th>aian</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98909</th>\n",
       "      <td>erdrich</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98910</th>\n",
       "      <td>egues</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98911</th>\n",
       "      <td>dotan</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98912</th>\n",
       "      <td>dionizio</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98913</th>\n",
       "      <td>donlea</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lastname  white  asian   mix  aian  black  hispanic\n",
       "98909   erdrich   0.74    0.0  0.05  0.12    0.0      0.09\n",
       "98910     egues   0.06    0.0  0.00  0.00    0.0      0.93\n",
       "98911     dotan   0.88    0.0  0.00  0.00    0.0      0.08\n",
       "98912  dionizio   0.92    0.0  0.00  0.00    0.0      0.07\n",
       "98913    donlea   0.94    0.0  0.00  0.00    0.0      0.06"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lastname_race_df = util.get_race_dist_of_lastname()\n",
    "lastname_race_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter on american artists only\n",
    "american_artist_df = artist_df[artist_df['nationality'] == 'American']\n",
    "non_american_artist_df = artist_df[artist_df['nationality'] != 'American']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiata/anaconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "\"\"\" COMPUTATIONALLY EXPENSIVE \"\"\"\n",
    "# create mapping with `lastname_race_df` to find assign a race to each artists\n",
    "american_artist_df['race'] = american_artist_df.apply(lambda row: pd.Series(util.get_race_from_full_name(row['artist_name'], row['race'], lastname_race_df)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_df = american_artist_df.append(non_american_artist_df)\n",
    "artist_df['exhibitions'] = artist_df['exhibitions'].apply(pd.to_numeric, errors='ignore')\n",
    "artist_df = artist_df.sort_values(by='exhibitions', ascending=False).reset_index().drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually go through all artists who have exhibited in the years 1957, 1977, 1997, & 2017 and check that the race and gender assignment for each of the artists are correct. If they are not, manually correct it.\n",
    "\n",
    "In total, there are ~1430 artists who have exhibited in 1957, 1977, 1997, & 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_race_and_gender(row):\n",
    "    \"\"\"\n",
    "    manual checked american & non-american artists of years: 1957, 1977, 1997 & 2017\n",
    "    \"\"\"\n",
    "    \n",
    "    white_male_list = [\\\n",
    "        \"sol lewitt\", \"ellsworth kelly\", \"edward ruscha\", \"john marin\", \"philip guston\", \"jonathan borofsky\", \\\n",
    "        \"william brice\", \"robert morris\", \"ben shahn\", \"e. mcknight kauffer\", \"robert andrew parker\", \"jan müller\", \\\n",
    "        \"jules pascin\", \"william t. wiley\", \"mark rothko\", \"varujan boghosian\", \"raoul hague\", \"joseph glasco\", \\\n",
    "        \"robert wilson\", \"garry winogrand\", \"albert alcalay\", \"gandy brodie\", \"frank lloyd wright\", \"raimund abraham\", \\\n",
    "        \"morris graves\", \"christopher wool\", \"merce cunningham\", \"louis michel eilshemius\", \"robert brownjohn\", \\\n",
    "        \"woody vasulka\", \"sam francis\",\"robert indiana\", \"william wegman\", \"gordon matta-clark\", \"mel bochner\", \\\n",
    "        \"willem de kooning\", \"milton avery\", \"felix gonzalez-torres\", \"douglas huebler\", \"r. buckminster fuller\", \\\n",
    "        \"tom wesselmann\", \"terry allen\", \"william bailey\", \"robert mapplethorpe\", \"richard foreman\", \"rafael ferrer\", \\\n",
    "        \"pirkle jones\", \"philippe halsman\", \"peter campus\", \"charles atlas\", \"douglas davis\", \"caldecot chubb\", \\\n",
    "        \"ben schonzeit\", \"jared bark\", \"jerry uelsmann\", \"abraham walkowitz\", \"peter young\", \"roger brown\", \\\n",
    "        \"tim rollins\", \"leon polk smith\", \"david park\", \"charles fahlen\", \"barry le va\", \"ian (hugh guiler) hugo\", \\\n",
    "        \"king vidor\", \"walter lang\", \"walter burley griffin\", \"val telberg\", \"paul taylor\", \"r. crumb\", \"remy charlip\", \\\n",
    "        \"robert breer\", \"bruce graham\", \"busby berkeley\", \"charles gwathmey\", \"charles j. brabin\", \"david levinthal\", \\\n",
    "        \"albert herbert\", \"allan mccollum\", \"eugene masselink\", \"george cukor\", \"willy mucha\", \"théo van rysselberghe\", \\\n",
    "        \"théophile-alexandre steinlen\", \"rené magritte\", \"hans namuth\", \"tom wesselmann\", \"mark di suvero\", \\\n",
    "        \"robert watts\", \"abraham walkowitz\", \"russell lee\", \"robert capa\", \"barry le va\", \"jerry uelsmann\", \\\n",
    "        \"george nelson\", \"joel meyerowitz\", \"william lescaze\", \"francis bruguière\", \"philip evergood\", \"jim shaw\", \\\n",
    "        \"george him\", \"günther förg\", \"rené robert bouché\", \"esteban vicente\", \"peter grippe\", \"mark grotjahn\", \\\n",
    "        \"john hejduk\", \"cornell capa\", \"robert gwathmey\", \"robert heinecken\", \"ed emshwiller\", \"donald sultan\", \\\n",
    "        \"gregory amenoff\", \"nathan george horwitt\", \"maurice sterne\", \"jean charlot\", \"timothy o'sullivan\", \\\n",
    "        \"alton pickens\", \"louis faurer\", \"richard neutra\", \"louis lozowick\", \"alfred leslie\", \"morris louis\", \\\n",
    "        \"kim jones\", \"carleton e. watkins\", \"william vandivert\", \"jules olitski\", \"louis schanker\", \\\n",
    "        \"walter dorwin teague\", \"edward kienholz\", \"nathan lyons\", \"walter robinson\", \"larry poons\", \"john steuart curry\", \\\n",
    "        \"alphonse mucha\", \"robert laurent\", \"bill beckley\", \"alfred eisenstaedt\", \"harwell hamilton harris\", \\\n",
    "        \"thomas wilfred\", \"ron davis\", \"geoffrey hendricks\", \"alain kirili\", \"john szarkowski\", \\\n",
    "        \"henry hobson richardson\"]\n",
    "    \n",
    "    white_female_list = [\\\n",
    "        \"helen frankenthaler\", \"lee bontecou\", \"yvonne rainer\", \"imogen cunningham\", \"charmion von wiegand\", \"angelo testa\" \\\n",
    "        \"louise nevelson\", \"adrian piper\", \"elizabeth murray\", \"mona hatoum\", \"lee krasner\", \"lois long\", \\\n",
    "        \"claire (claire mahl) moore\", \"alexis smith\", \"trisha brown\", \"lucinda childs\", \"margaret c. anderson\", \\\n",
    "        \"vera (vera neumann)\", \"susan weil\", \"noémi raymond\", \"pat passlof\", \"jan groover\", \"elaine de kooning\", \\\n",
    "        \"nancy holt\", \"susan weil\", \"wanda gág\", \"elizabeth peyton\", \"judith joy ross\", \"louise dahl-wolfe\", \\\n",
    "        \"jane dickson\"]\n",
    "    \n",
    "    asian_female_list = [\\\n",
    "        \"elizabeth mcfadden\", \"tomiyo sasaki\"]\n",
    "    \n",
    "    asian_male_list = [\\\n",
    "        \"shusaku arakawa\", \"lee ufan\", \"wifredo lam\", \"ai weiwei\", \"chinn yuen-yuei\", \"thomas han\", \\\n",
    "        \"jenova (xinghan) chen\", \"eikoh hosoe\", \"ken domon\", \"lee jong-ok\"]\n",
    "    \n",
    "    black_male_list = [\\\n",
    "        \"david hammons\", \"kingelez\", \"raymond saunders\", \"melvin edwards\", \"sam gilliam\", \"Cameron Rowland\", \\\n",
    "        \"terry adkins\", \"jacob lawrence\", \"gordon parks\"]\n",
    "    \n",
    "    black_female_list = [\\\n",
    "        \"minnie evans\", \"kara walker\", \"xaviera simmons\", \"carrie mae weems\", \"alma woodsey thomas\", \\\n",
    "        \"barbara chase-riboud\"]\n",
    "    \n",
    "    hispanic_female_list = [\\\n",
    "        \"andrea bowers\", \"andrea fraser\"]\n",
    "    \n",
    "    hispanic_male_list = [\\\n",
    "        \"rufino tamayo\"]\n",
    "    \n",
    "    none_list = [\\\n",
    "        \"velox ward\", \"dudley huppler\", \"schilli maier\", \"maxi cohen\", \"richard w. landis\", \"skip blumberg\", \\\n",
    "        \"joel fisher\", \"eve sonneman\", \"mia ferrara\", \"john h. lickert\", \"william c. gannett\", \"robert p. gottlieb\", \\\n",
    "        \"orlando giannini\", \"daniel larossa\", \"alfred w. fielding\", \"arthur a. aykanian\", \"don weinreich\", \\\n",
    "        \"janet stein\", \"eliza montgomery\", \"elizabeth mock\", \"roland baladi\"]\n",
    "    \n",
    "    artist_name = str(row['artist_name'].lower())\n",
    "    if \"nknown\" in artist_name \\\n",
    "        or \"nonymous\" in artist_name \\\n",
    "        or \"rtist\" in artist_name \\\n",
    "        or \"ystem\" in artist_name \\\n",
    "        or \"group\" in artist_name \\\n",
    "        or \"corp\" in artist_name \\\n",
    "        or \", \" in artist_name \\\n",
    "        or \"ssociates\" in artist_name \\\n",
    "        or \"tudio\" in artist_name \\\n",
    "        or \"esearch\" in artist_name \\\n",
    "        or \"imension\" in artist_name \\\n",
    "        or \"skidmore\" in artist_name \\\n",
    "        or \"rchitect\" in artist_name \\\n",
    "        or artist_name in none_list:\n",
    "        return None, None\n",
    "    \n",
    "    if artist_name in white_male_list:\n",
    "        return \"white\", \"male\"\n",
    "    \n",
    "    if artist_name in white_female_list:\n",
    "        return \"white\", \"female\"\n",
    "    \n",
    "    if artist_name in asian_female_list:\n",
    "        return \"asian\", \"female\"\n",
    "    \n",
    "    if artist_name in asian_male_list:\n",
    "        return \"asian\", \"male\"\n",
    "    \n",
    "    if artist_name in black_male_list:\n",
    "        return \"black\", \"male\"\n",
    "    \n",
    "    if artist_name in black_female_list:\n",
    "        return \"black\", \"female\"\n",
    "    \n",
    "    if artist_name in hispanic_female_list:\n",
    "        return \"hispanic\", \"female\"\n",
    "    \n",
    "    if artist_name in hispanic_male_list:\n",
    "        return \"hispanic\", \"male\"\n",
    "    \n",
    "    return row['race'], row['gender']\n",
    "\n",
    "artist_df[['race', 'gender']] = artist_df.apply(lambda x: pd.Series(check_race_and_gender(x)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that dataframes look as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(artist_df.shape)\n",
    "artist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exhibition_df.shape)\n",
    "exhibition_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataframes as pickled files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_df.to_pickle('data/artist_%s.pkl' % str(time.strftime(\"%m%d%Y\")))\n",
    "exhibition_df.to_pickle('data/exhibition_%s.pkl' % str(time.strftime(\"%m%d%Y\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
